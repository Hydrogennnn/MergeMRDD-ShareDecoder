
train:
  epochs: 200
  # 64, 128, 512, 1024
  devices: [0]
  batch_size: 256
  optim: "adamw"
  num_workers: 8
  lr: 0.0005
  evaluate: 1
  use_ddp: false
  scheduler: 'consine'
  samples_num: 8
  resume: false
  masked_ratio: 0.7
  mask_patch_size: 4

